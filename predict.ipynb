{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da6c0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbf11580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_json(json_path):\n",
    "    f = open(json_path, 'r', encoding='utf-8')\n",
    "    json_data = json.load(f)\n",
    "    f.close()\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "944e2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421\n"
     ]
    }
   ],
   "source": [
    "mode = 'public'\n",
    "json_path = f'./data/{mode}_complete.json'\n",
    "json_data = load_data_json(json_path)\n",
    "print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ee60554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421\n"
     ]
    }
   ],
   "source": [
    "results_path = f'./output_results/public_result_colbert_title_allkeywords_epoch1'\n",
    "with open(results_path, 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe33f718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "data_mode = 'public'\n",
    "binary_used_epoch = 2\n",
    "# binary_results_paths = [\n",
    "#     f'./output_results/{data_mode}_binary_result_cross_encoder_keywords_title_text_large_ensemble1_epoch{binary_used_epoch}',\n",
    "#     f'./output_results/{data_mode}_binary_result_cross_encoder_keywords_title_text_large_ensemble2_epoch{binary_used_epoch}',\n",
    "#     f'./output_results/{data_mode}_binary_result_cross_encoder_keywords_title_text_large_ensemble3_epoch{binary_used_epoch}',\n",
    "#     f'./output_results/{data_mode}_binary_result_cross_encoder_keywords_title_text_large_ensemble4_epoch{binary_used_epoch}',\n",
    "#     f'./output_results/{data_mode}_binary_result_cross_encoder_keywords_title_text_large_ensemble5_epoch{binary_used_epoch}',\n",
    "# ]\n",
    "\n",
    "binary_results_paths = [\n",
    "    'output_results/public_binary_result_cross_encoder_keywords_title_text_large_ensemble_alldata1',\n",
    "    'output_results/public_binary_result_cross_encoder_keywords_title_text_large_ensemble_alldata2',\n",
    "    'output_results/public_binary_result_cross_encoder_keywords_title_text_large_ensemble_alldata3',\n",
    "    'output_results/public_binary_result_cross_encoder_keywords_title_text_large_ensemble_alldata4',\n",
    "    'output_results/public_binary_result_cross_encoder_keywords_title_text_large_ensemble_alldata5',\n",
    "\n",
    "    'output_results/public_binary_result_cross_encoder_20_40_keywords_title_text_large_ensemble1',\n",
    "    'output_results/public_binary_result_cross_encoder_20_40_keywords_title_text_large_ensemble2',\n",
    "    'output_results/public_binary_result_cross_encoder_20_40_keywords_title_text_large_ensemble3',\n",
    "    'output_results/public_binary_result_cross_encoder_20_40_keywords_title_text_large_ensemble4',\n",
    "    'output_results/public_binary_result_cross_encoder_20_40_keywords_title_text_large_ensemble5',\n",
    "    \n",
    "    'output_results/public_binary_result_chinese-macbert-large_hard4_rand6_alldata_ensemble1',\n",
    "    'output_results/public_binary_result_chinese-macbert-large_hard4_rand6_alldata_ensemble2',\n",
    "    'output_results/public_binary_result_chinese-macbert-large_hard4_rand6_alldata_ensemble3',\n",
    "    \n",
    "    'output_results/public_binary_result_chinese-macbert-large_hard4_rand6_title_and_text_fixkeywords_ensemble1',\n",
    "    'output_results/public_binary_result_chinese-macbert-large_hard4_rand6_title_and_text_fixkeywords_ensemble2',\n",
    "    'output_results/public_binary_result_chinese-macbert-large_hard4_rand6_title_and_text_fixkeywords_ensemble3',\n",
    "\n",
    "# #     'output_results/public_binary_result_chinese-roberta-wwm-ext-large_hard4_rand6_alldata_ensemble1',\n",
    "# #     'output_results/public_binary_result_chinese-roberta-wwm-ext-large_hard4_rand6_alldata_ensemble2',\n",
    "# #     'output_results/public_binary_result_chinese-roberta-wwm-ext-large_hard4_rand6_alldata_ensemble3',\n",
    "    \n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard5_rand5_title_and_text_fixkeywords_alldata_ensemble1',\n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard5_rand5_title_and_text_fixkeywords_alldata_ensemble2',\n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard5_rand5_title_and_text_fixkeywords_alldata_ensemble3',\n",
    "    \n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard10_rand20_oversample_title_and_text_fixkeywords_ensemble1',\n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard10_rand20_oversample_title_and_text_fixkeywords_ensemble2',\n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard10_rand20_oversample_title_and_text_fixkeywords_ensemble3',\n",
    "    \n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard20_rand20_title_and_text_fixkeywords_ensemble1',\n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard20_rand20_title_and_text_fixkeywords_ensemble2',\n",
    "    './output_results/public_binary_result_chinese-macbert-large_hard20_rand20_title_and_text_fixkeywords_ensemble3',\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "num_of_results = len(binary_results_paths)\n",
    "print(num_of_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e97d2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n",
      "421\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for binary_result_path in binary_results_paths:\n",
    "# binary_result_path  = f'./output_results/public_binary_result_cross_encoder_keywords_title_text_large_ensemble1_epoch2'\n",
    "    with open(binary_result_path, 'rb') as handle:\n",
    "        binary_results = pickle.load(handle)\n",
    "    print(len(binary_results))\n",
    "    all_results.append(binary_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77c04f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421\n"
     ]
    }
   ],
   "source": [
    "ensemble_results = {}\n",
    "for did in all_results[0].keys():\n",
    "    all_scores = []\n",
    "    for binary_result in all_results:\n",
    "        binary_scores = []\n",
    "        ref_dids = []\n",
    "        for ref_did, binary_score in binary_result[did]:\n",
    "            binary_scores.append(binary_score)\n",
    "            ref_dids.append(ref_did)\n",
    "\n",
    "        all_scores.append(binary_scores)\n",
    "\n",
    "    all_scores = np.array(all_scores)\n",
    "    all_scores = np.sum(all_scores, axis=0)\n",
    "    all_scores = all_scores / num_of_results\n",
    "    \n",
    "\n",
    "    did_result = [(ref_did, score) for ref_did, score in zip(ref_dids, all_scores)]\n",
    "    ensemble_results[did] = did_result\n",
    "\n",
    "binary_results = ensemble_results\n",
    "print(len(binary_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "776ddaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_result_path  = f'./output_results/public_binary_result_cross_encoder_keywords_title_text_large_ensemble1_epoch2'\n",
    "# with open(binary_result_path, 'rb') as handle:\n",
    "#     binary_results = pickle.load(handle)\n",
    "    \n",
    "# print(len(binary_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2505a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "did2data = {}\n",
    "for d in json_data:\n",
    "    did = d['did']\n",
    "    did2data[did] = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0413e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.summarization import bm25\n",
    "# corpus_ids = [d['did'] for d in json_data]\n",
    "# corpus = []\n",
    "# titles = []\n",
    "# title_and_corpus = []\n",
    "\n",
    "# from zhon.hanzi import punctuation\n",
    "# import string\n",
    "# for i, d in enumerate(json_data):\n",
    "#     text_words = d['replaced_text_sentence'].split()\n",
    "#     title_words = d['replaced_title_sentence'].split()\n",
    "\n",
    "#     filter_title_words = [w for w in title_words if w not in punctuation and w not in string.punctuation]\n",
    "\n",
    "#     filter_text_words = [w for w in text_words if w not in punctuation and w not in string.punctuation]\n",
    "\n",
    "#     corpus.append(filter_text_words)\n",
    "#     titles.append(filter_title_words)\n",
    "#     title_and_corpus.append(filter_title_words + filter_text_words)\n",
    "\n",
    "# bm25Model = bm25.BM25(title_and_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ece604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, query in enumerate((title_and_corpus)):\n",
    "#     q_did = corpus_ids[i]\n",
    "#     result = results[q_did]\n",
    "# #     bm25_scores = []\n",
    "#     scores = bm25Model.get_scores(query)\n",
    "#     for idx, score in enumerate(scores):\n",
    "#         ref_did = corpus_ids[idx]\n",
    "#         if q_did == ref_did:\n",
    "#             continue\n",
    "#         for k, r in enumerate(result):\n",
    "#             if r[0] == ref_did:\n",
    "#                 r = r + (score,)\n",
    "#                 result[k] = r\n",
    "#                 break\n",
    "#     results[q_did] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7682845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "# # files = glob.glob(\"./data/Keywords/*.xlsx\")\n",
    "# files = glob.glob(\"./data/Keywords/02pest.list.xlsx\")\n",
    "# same_words_dict = {}\n",
    "\n",
    "# other_keywords = [\n",
    "#     '葡萄露菌病',\n",
    "#     '白葉枯病'\n",
    "#     '舞蛾',\n",
    "# ]\n",
    "\n",
    "# for file in files:\n",
    "#     df = pd.read_excel(file, header=None)\n",
    "#     df_list = df.values.tolist()\n",
    "#     for words in df_list:\n",
    "#         change_to = words[0]\n",
    "#         same_words_dict[change_to] = change_to\n",
    "#         for word in words:\n",
    "#             word = str(word)\n",
    "#             if word == 'nan':\n",
    "#                 break\n",
    "#             if word != change_to:\n",
    "#                 same_words_dict[word] = change_to\n",
    "\n",
    "# for other_keyword in other_keywords:\n",
    "#     same_words_dict[other_keyword] = other_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09ab40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # keyword_sets = []\n",
    "# for i, d in enumerate(json_data):\n",
    "#     did = d['did']\n",
    "#     text_words = d['text_sentence'].split()\n",
    "#     title_words = d['title_sentence'].split()\n",
    "#     all_words = text_words + title_words\n",
    "#     keyword_set = set()\n",
    "#     for k, v in same_words_dict.items():\n",
    "#         if k in all_words:\n",
    "#             keyword_set.add(same_words_dict[k])\n",
    "# #     text = ' '.join(all_words)\n",
    "# #     key_phrases = jio.keyphrase.extract_keyphrase(text)\n",
    "# #     for key_phrase in key_phrases:\n",
    "# #         if key_phrase in same_words_dict:\n",
    "# #             keyword_set.add(same_words_dict[key_phrase])\n",
    "# #         else:\n",
    "# #             keyword_set.add(key_phrase)\n",
    "# #     keyword_sets.append(keyword_set)\n",
    "#     json_data[i]['keyword_set'] = keyword_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "484d448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_score(set1, set2):\n",
    "    u = set1.union(set2) \n",
    "    i = set1.intersection(set2)\n",
    "    if len(u) == 0:\n",
    "        return 0\n",
    "    return len(i) / len(u)\n",
    "\n",
    "def dsc_score(set1, set2):\n",
    "    i = set1.intersection(set2)\n",
    "    if len(set1) + len(set2) == 0:\n",
    "        return 0\n",
    "    return (2 * len(i)) / (len(set1) + len(set2))\n",
    "\n",
    "def overlap_coefficient(set1, set2):\n",
    "    i = set1.intersection(set2)\n",
    "    min_ = min(len(set1), len(set2))\n",
    "    if min_ == 0:\n",
    "        return 0\n",
    "    return len(i) / min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6fa79075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "keyword_set_sim = defaultdict(dict)\n",
    "for i in range(len(json_data)):\n",
    "    for k in range(len(json_data)):\n",
    "        \n",
    "        did_i = json_data[i]['did']\n",
    "        did_k = json_data[k]['did']\n",
    "        \n",
    "        set_i = set(json_data[i]['keyword_set'])\n",
    "        set_k = set(json_data[k]['keyword_set'])\n",
    "        \n",
    "        sim_socre = overlap_coefficient(set_i, set_k)\n",
    "        keyword_set_sim[did_i][did_k] = sim_socre\n",
    "\n",
    "for i in range(len(json_data)):\n",
    "    did_i = json_data[i]['did']\n",
    "    result = binary_results[did_i]\n",
    "    set_sim_score = keyword_set_sim[did_i]\n",
    "    for ref_did, score in set_sim_score.items():\n",
    "        if did_i == ref_did:\n",
    "            continue\n",
    "        for k, r in enumerate(result):\n",
    "            if r[0] == ref_did:\n",
    "                r = r + (score,)\n",
    "                result[k] = r\n",
    "                break\n",
    "    binary_results[did_i] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e16396b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_pairs = []\n",
    "# binary_th = 0.99\n",
    "\n",
    "# for k, values in all_results[0].items():\n",
    "#     did = str(k)\n",
    "\n",
    "#     all_ref_ids = np.array([v[0] for v in values])\n",
    "\n",
    "#     all_preds = []\n",
    "#     for single_binary_result in all_results:\n",
    "\n",
    "#         binary_value = single_binary_result[k]\n",
    "\n",
    "#         binary_scores = [v[1] for v in binary_value]\n",
    "#         binary_scores = np.array(binary_scores)\n",
    "#         binary_pos_idx = binary_scores >= binary_th\n",
    "#         binary_preds = all_ref_ids[binary_pos_idx]\n",
    "#         binary_pos_scores = binary_scores[binary_pos_idx]\n",
    "\n",
    "#         all_preds.append(set(binary_preds))\n",
    "# #     preds = set.intersection(*all_preds)\n",
    "#     preds = set.union(*all_preds)\n",
    "\n",
    "#     for pred in preds:\n",
    "#         pos_pairs.append((did, pred))\n",
    "    \n",
    "# pos_pairs = list(set(pos_pairs))\n",
    "# print(len(pos_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc05a916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topk = 20\n",
    "\n",
    "pos_pairs = []\n",
    "\n",
    "second_num_of_std = 2\n",
    "\n",
    "num_of_std = 3.2\n",
    "binary_th = 0.6\n",
    "set_th = 0.6\n",
    "all_scores = []\n",
    "for k, values in binary_results.items():\n",
    "    did = str(k)\n",
    "    \n",
    "    binary_value = binary_results[k]\n",
    "    all_ref_ids = np.array([v[0] for v in values])\n",
    "    data = did2data[did]\n",
    "    pos_dids = data['pos_dids']\n",
    "        \n",
    "#     print(f'did = {k}')\n",
    "    \n",
    "# #     bm25_pos_dids = data['bm25_pos_dids'][:topk]\n",
    "#     bm25_scores = [v[2] for v in values]\n",
    "#     bm25_scores = np.array(bm25_scores)\n",
    "    \n",
    "# #     bm25_scores_normalize = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))\n",
    "\n",
    "    \n",
    "#     bm25_mean = np.mean(bm25_scores)\n",
    "#     bm25_median = np.median(bm25_scores)\n",
    "#     bm25_std = np.std(bm25_scores)\n",
    "#     bm25_max_score = np.max(bm25_scores)\n",
    "#     bm25_scores_normalize = (bm25_scores - bm25_mean) / bm25_std\n",
    "\n",
    "#     bm25_th = bm25_median + num_of_std * bm25_std\n",
    "# #     bm25_pos_idx = bm25_scores >= bm25_th\n",
    "#     bm25_pos_idx = bm25_scores_normalize >= 4\n",
    "#     bm25_preds = all_ref_ids[bm25_pos_idx]\n",
    "    \n",
    "    \n",
    "    \n",
    "    set_scores =  [v[2] for v in values]\n",
    "    set_scores = np.array(set_scores)\n",
    "#     set_pos_idx = set_scores >= 0.8\n",
    "#     set_preds = all_ref_ids[set_pos_idx]\n",
    "#     input()\n",
    "    \n",
    "    \n",
    "#     scores = [v[1] for v in values]\n",
    "#     scores = np.array(scores)\n",
    "# #     scores_normalize = (scores - np.min(scores)) / (np.max(scores) - np.min(scores))\n",
    "#     mean = np.mean(scores)\n",
    "#     median = np.median(scores)\n",
    "#     std = np.std(scores)\n",
    "#     scores_normalize = (scores - mean) / std\n",
    "#     th = mean + num_of_std * std\n",
    "#     pos_idx = scores_normalize >= 3.5\n",
    "#     pos_ref_ids = all_ref_ids[pos_idx]\n",
    "#     pos_scores = scores[pos_idx]\n",
    "\n",
    "    \n",
    "    binary_scores = [v[1] for v in binary_value]\n",
    "    binary_scores = np.array(binary_scores)\n",
    "    binary_pos_idx = binary_scores >= binary_th\n",
    "    binary_preds = all_ref_ids[binary_pos_idx]\n",
    "    binary_pos_scores = binary_scores[binary_pos_idx]\n",
    "    \n",
    "    \n",
    "    mix_scores = set_scores + binary_scores\n",
    "    mix_pos_idx = mix_scores >= 1.4\n",
    "    mix_preds = all_ref_ids[mix_pos_idx]\n",
    "\n",
    "# #     print(pos_ref_ids)\n",
    "# #     print(pos_scores)\n",
    "    \n",
    "#     secnod_pred_mean = np.mean(pos_scores)\n",
    "#     second_median = np.median(pos_scores)\n",
    "#     second_std = np.std(pos_scores)\n",
    "#     second_th = second_median + second_num_of_std * second_std\n",
    "#     second_pos_idx = pos_scores >=  second_th\n",
    "#     second_preds = pos_ref_ids[second_pos_idx]\n",
    "#     second_pos_scores = pos_scores[second_pos_idx]\n",
    "# #     print('second pred')\n",
    "# #     print(second_preds)\n",
    "#     print(second_pos_scores)\n",
    "\n",
    "#     all_scores += list(binary_pos_scores)\n",
    "    preds = binary_preds\n",
    "    \n",
    "#     print(preds)\n",
    "    \n",
    "    for pred in preds:\n",
    "        pos_pairs.append((did, pred))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9fea86cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63bb80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_static_info(li):\n",
    "#     mean = np.mean(li)\n",
    "#     median = np.median(li)\n",
    "#     std = np.std(li)\n",
    "#     max_ = np.max(li)\n",
    "#     min_ = np.min(li)\n",
    "#     print(f' mean {mean} median {median}, std {std}, max {max_}, min {min_}')\n",
    "\n",
    "# print('all score')\n",
    "# get_static_info(all_scores)\n",
    "# print(len(all_scores))\n",
    "\n",
    "# n, bins, patches = plt.hist(all_scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfc0a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submit_path = './submit/macbert-super-th06.csv'\n",
    "df = pd.DataFrame(pos_pairs, columns=['Test','Reference'])\n",
    "df.to_csv(submit_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45db51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
